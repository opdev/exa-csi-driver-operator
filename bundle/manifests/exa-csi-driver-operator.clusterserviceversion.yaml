apiVersion: operators.coreos.com/v1alpha1
kind: ClusterServiceVersion
metadata:
  annotations:
    alm-examples: |-
      [
        {
          "apiVersion": "charts.my.domain/v1alpha1",
          "kind": "ExaCsi",
          "metadata": {
            "name": "exacsi-sample"
          },
          "spec": {
            "image": {
              "pullPolicy": "Always",
              "repository": "quay.io/ddn/exascaler-csi-file-driver",
              "tag": "master"
            },
            "namespace": "exa-csi-driver-operator-system",
            "replicaCount": 1,
            "resources": {
              "limits": {
                "cpu": "100m",
                "memory": "128Mi"
              },
              "requests": {
                "cpu": "100m",
                "memory": "128Mi"
              }
            },
            "secretName": "exascaler-csi-file-driver-config",
            "volumeNamePrefix": "pvc-exa"
          }
        }
      ]
    capabilities: Basic Install
    categories: Storage
    createdAt: "2023-11-09T16:09:21Z"
    operators.operatorframework.io/builder: operator-sdk-v1.28.1
    operators.operatorframework.io/project_layout: helm.sdk.operatorframework.io/v1
    repository: https://github.com/DDNStorage/exa-csi-driver
  name: exa-csi-driver-operator.v0.0.1
  namespace: placeholder
spec:
  apiservicedefinitions: {}
  customresourcedefinitions:
    owned:
    - kind: ExaCsi
      name: exacsis.charts.my.domain
      version: v1alpha1
  description: |
    A Container Storage Interface (CSI) Driver for DDN EXAScaler filesystems. This operator eases the installation
    and management of the EXAScaler CSI Driver in an OpenShift or other Operator Lifecycle Manager enabled cluster.

    ## Prerequisites
    The Exa CSI Driver requires installing the lustre kernel module onto your cluster nodes. OpenShifts supports installing kernel modules using the Kernel Module Manager Operator (KMM).

    KMM uses a ModuleLoader image to load kernel modules onto cluster nodes. You can build a ModuleLoader image outside your cluster or you can create a ConfigMap containing a Dockerfile which KMM will use to build and store the image on your cluster. Please see the [documentation for creating ModuleLoader images here](https://openshift-kmm.netlify.app/documentation/module_loader_image/) for more information.
    
    You must also make sure to install the proper lustre version. To match the lustre package versions to your OpenShift release, please cross-reference [the Lustre version support matrix](https://wiki.whamcloud.com/display/PUB/Lustre+Support+Matrix)  with the [RHEL version utilized by your OpenShift release](https://access.redhat.com/articles/6907891). Once you know the version of lustre that matches your OpenShift cluster, you will want to note the URLs of the `lustre-client` and `kmod-lustre-client` packages hosted [here](https://downloads.whamcloud.com/public/lustre). You can export these values in a shell session to simplify the following instructions. For example, if we want lustre 2.15.3 for OpenShift 4.13:
    ```bash
    export LUSTRE_PACKAGE=https://downloads.whamcloud.com/public/lustre/latest-release/el9.2/client/RPMS/x86_64/lustre-client-2.15.3-1.el9.x86_64.rpm
    export KMOD_PACKAGE=https://downloads.whamcloud.com/public/lustre/latest-release/el9.2/client/RPMS/x86_64/kmod-lustre-client-2.15.3-1.el9.x86_64.rpm
    ```

    Building images on your cluster will also require [properly configured Red Hat entitlements on your cluster](https://examples.openshift.pub/cluster-configuration/full-cluster-entitlement/).
    
    ### Lustre Kernel Module Quick Setup
    - Install the Kernel Module Management operator through the OperatorHub if it is not already installed.
    - Retrieve a sample ConfigMap for building the ModuleLoader image and substitute the URLs of the `lustre-client` and `kmod-lustre-client` packages that match your OpenShift release into the sample ConfigMap:
      ```bash
      wget sample-lustre-dockerfile-configmap.yaml
      sed -i "s|LUSTRE_PACKAGE|$LUSTRE_PACKAGE|g" sample-lustre-dockerfile-configmap.yaml
      sed -i "s|KMOD_PACKAGE|$KMOD_PACKAGE|g" sample-lustre-dockerfile-configmap.yaml
      ```

    - Add the ConfigMap to your cluster in the `openshift-kmm` default namespace: 
    ```bash
    oc apply -n openshift-kmm -f sample-lustre-dockerfile-configmap.yaml
    ```

    - Retrieve a sample DaemonSet that runs necessary network configuration for the kernel modules. If you know a particular interface configuration that you require, substitute that into the `lnetctl net add` line of the file (our example uses the default `br-ex` interface used by the OVN-Kubernetes network provider):
    ```bash
    wget lnet-configuration-ds.yaml
    # if you want to change the network interface used:
    sed -i "s/br-ex/my_interface/g" lnet-configuration-ds.yaml
    ```

    -  Retrieve the `lnet` Module Custom Resource and apply it and the DaemonSet to your cluster in the `openshift-kmm` namespace:
    ```bash
    wget lnet-mod.yaml
    oc apply -n openshift-kmm -f lnet-mod.yaml -f lnet-configuration-ds.yaml
    ```

    - Adding the `lnet` Module CR to your cluster will trigger a build of the image in the previously applied ConfigMap. You can monitor the status of the build in the OpenShift Console or on the command line:
    ```bash
    oc get -n openshift-kmm builds -w
    ```

    - Shortly after the build completes, you should see the status of the child pods of your Module and DaemonSet change to "Running":
    ```bash
    oc get -n openshift-kmm pods

    # Output
    NAME                                               READY   STATUS    RESTARTS      AGE
    kmm-operator-controller-manager-6bfc986b4b-22xv9   2/2     Running   4 (10h ago)   8d
    lnet-452sc-85rs4                                   1/1     Running   0             57m
    lnet-configuration-c6d6w                           1/1     Running   0             57m
    ```

    - Retrieve and apply the `lustre` Module CR to your cluster:
    ```bash
    wget lustre-mod.yaml
    oc apply -n openshift-kmm -f lustre-mod.yaml
    ```

    - To verify the modules are loaded and the configuration is set correctly on any particular node, you can open a debug session using your ModuleLoader image:
    ```bash
    oc debug node/my-node

    # Once in the debug pod session
    lsmod | grep lustre # Should show the lustre kernel module is loaded

    lnetctl net show # Should show a tcp interface configured on your interface, br-ex by default

    # Example output:
    net:
        - net type: lo
          local NI(s):
            - nid: 0@lo
              status: up
        - net type: tcp
          local NI(s):
            - nid: 10.0.14.194@tcp
              status: up
              interfaces:
                  0: br-ex

    ```

  displayName: EXAScaler CSI File Driver
  icon:
  - base64data: ""
    mediatype: ""
  install:
    spec:
      clusterPermissions:
      - rules:
        - apiGroups:
          - ""
          resources:
          - namespaces
          verbs:
          - get
        - apiGroups:
          - ""
          resources:
          - secrets
          verbs:
          - '*'
        - apiGroups:
          - ""
          resources:
          - events
          verbs:
          - create
        - apiGroups:
          - charts.my.domain
          resources:
          - exacsis
          - exacsis/status
          - exacsis/finalizers
          verbs:
          - create
          - delete
          - get
          - list
          - patch
          - update
          - watch
        - apiGroups:
          - ""
          resources:
          - pods
          - services
          - services/finalizers
          - endpoints
          - persistentvolumeclaims
          - events
          - configmaps
          - secrets
          - serviceaccounts
          verbs:
          - create
          - delete
          - get
          - list
          - patch
          - update
          - watch
        - apiGroups:
          - apps
          resources:
          - deployments
          - daemonsets
          - replicasets
          - statefulsets
          verbs:
          - create
          - delete
          - get
          - list
          - patch
          - update
          - watch
        - apiGroups:
          - rbac.authorization.k8s.io
          resources:
          - clusterroles
          - clusterrolebindings
          - roles
          - rolebindings
          verbs:
          - create
          - delete
          - get
          - list
          - patch
          - update
          - watch
        - apiGroups:
          - storage.k8s.io
          resources:
          - csidrivers
          - volumeattachments
          verbs:
          - create
          - delete
          - get
          - list
          - patch
          - update
          - watch
        - apiGroups:
          - ""
          resources:
          - nodes
          verbs:
          - get
          - list
          - watch
        - apiGroups:
          - ""
          resources:
          - persistentvolumeclaims/status
          verbs:
          - update
          - patch
        - apiGroups:
          - apiextensions.k8s.io
          resources:
          - customresourcedefinitions
          verbs:
          - list
          - watch
          - create
          - delete
        - apiGroups:
          - ""
          resources:
          - persistentvolumes
          verbs:
          - create
          - delete
          - get
          - list
          - watch
          - update
          - patch
        - apiGroups:
          - snapshot.storage.k8s.io
          resources:
          - volumesnapshotcontents
          - volumesnapshots
          verbs:
          - get
          - list
          - watch
          - update
          - create
          - delete
        - apiGroups:
          - storage.k8s.io
          resources:
          - storageclasses
          verbs:
          - watch
        - apiGroups:
          - storage.k8s.io
          resources:
          - csinodes
          verbs:
          - watch
          - list
          - get
        - apiGroups:
          - csi.storage.k8s.io
          resources:
          - csinodeinfos
          verbs:
          - get
          - list
          - watch
        - apiGroups:
          - coordination.k8s.io
          resources:
          - leases
          verbs:
          - get
          - watch
          - list
          - delete
          - update
          - create
        - apiGroups:
          - security.openshift.io
          resourceNames:
          - privileged
          resources:
          - securitycontextconstraints
          verbs:
          - use
        - apiGroups:
          - authentication.k8s.io
          resources:
          - tokenreviews
          verbs:
          - create
        - apiGroups:
          - authorization.k8s.io
          resources:
          - subjectaccessreviews
          verbs:
          - create
        serviceAccountName: exa-csi-driver-operator-controller-manager
      deployments:
      - label:
          app.kubernetes.io/component: manager
          app.kubernetes.io/created-by: exa-csi-driver-operator
          app.kubernetes.io/instance: controller-manager
          app.kubernetes.io/managed-by: kustomize
          app.kubernetes.io/name: deployment
          app.kubernetes.io/part-of: exa-csi-driver-operator
          control-plane: controller-manager
        name: exa-csi-driver-operator-controller-manager
        spec:
          replicas: 1
          selector:
            matchLabels:
              control-plane: controller-manager
          strategy: {}
          template:
            metadata:
              annotations:
                kubectl.kubernetes.io/default-container: manager
              labels:
                control-plane: controller-manager
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: kubernetes.io/arch
                        operator: In
                        values:
                        - amd64
                        - arm64
                        - ppc64le
                        - s390x
                      - key: kubernetes.io/os
                        operator: In
                        values:
                        - linux
              containers:
              - args:
                - --secure-listen-address=0.0.0.0:8443
                - --upstream=http://127.0.0.1:8080/
                - --logtostderr=true
                - --v=0
                image: gcr.io/kubebuilder/kube-rbac-proxy:v0.13.1
                name: kube-rbac-proxy
                ports:
                - containerPort: 8443
                  name: https
                  protocol: TCP
                resources:
                  limits:
                    cpu: 500m
                    memory: 128Mi
                  requests:
                    cpu: 5m
                    memory: 64Mi
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
              - args:
                - --health-probe-bind-address=:8081
                - --metrics-bind-address=127.0.0.1:8080
                - --leader-elect
                - --leader-election-id=exa-csi-driver-operator
                image: quay.io/dhoover/exa-csi-driver-operator-controller:latest
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: 8081
                  initialDelaySeconds: 15
                  periodSeconds: 20
                name: manager
                readinessProbe:
                  httpGet:
                    path: /readyz
                    port: 8081
                  initialDelaySeconds: 5
                  periodSeconds: 10
                resources:
                  limits:
                    cpu: 500m
                    memory: 128Mi
                  requests:
                    cpu: 10m
                    memory: 64Mi
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop:
                    - ALL
              securityContext:
                runAsNonRoot: true
              serviceAccountName: exa-csi-driver-operator-controller-manager
              terminationGracePeriodSeconds: 10
      permissions:
      - rules:
        - apiGroups:
          - ""
          resources:
          - configmaps
          verbs:
          - get
          - list
          - watch
          - create
          - update
          - patch
          - delete
        - apiGroups:
          - coordination.k8s.io
          resources:
          - leases
          verbs:
          - get
          - list
          - watch
          - create
          - update
          - patch
          - delete
        - apiGroups:
          - ""
          resources:
          - events
          verbs:
          - create
          - patch
        serviceAccountName: exa-csi-driver-operator-controller-manager
    strategy: deployment
  installModes:
  - supported: true
    type: OwnNamespace
  - supported: true
    type: SingleNamespace
  - supported: true
    type: MultiNamespace
  - supported: true
    type: AllNamespaces
  keywords:
  - CSI
  - Lustre
  - EXAScaler
  - HPC
  - AI
  - High-Performance Computing
  links:
  - name: DDN EXAScaler Filesystem
    url: https://www.ddn.com/products/lustre-file-system-exascaler/
  maintainers:
  - email: dhoover@redhat.com
    name: dhoover
  maturity: alpha
  provider:
    name: DataDirect Networks
    url: https://www.ddn.com
  version: 0.0.1
